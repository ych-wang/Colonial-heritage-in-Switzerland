{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header \n",
    "Author : Amina Matt and Yichen Wang  \n",
    "Date created : 14.10.2021  \n",
    "Date last modified : 21.11.2021  \n",
    "Python version : 3.8  \n",
    "Description : Text processing of the CARICOM Compilation Archive (CCA) https://louverture.ch/cca/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "- [X] check number items\n",
    "- [X] to JSON \n",
    "- [ ] JSON person cleaning\n",
    "- [ ] JSON location cleaning\n",
    "- [ ] save NER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk #Natural Language Toolkit is a natural language programming library\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text separation into items \n",
    "In the primary text source, each item is separated by a return and the '=>' starting string. Each item references a different actor of colonial entreprise. Separating each of them into items helps us to differentiate the extraction depending on the scheme they follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: path for the .txt file \n",
    "#Output: list of string, where each element is an item, i.e. a separate entry in the document of origin\n",
    "#Requirements: -\n",
    "#Description: separate the items based on the '=>' string that characterize a new entry\n",
    "def divide_items(textFilePath):\n",
    "    f = open(textFilePath,\"r\")\n",
    "    item = []\n",
    "    for line in f: \n",
    "        if (line != '\\n'):\n",
    "            if (line[0] == '=') and (line[1] == '>'):\n",
    "                item_text = ''\n",
    "                while (line != '\\n'):\n",
    "                    item_text = item_text + line\n",
    "                    line = f.readline()\n",
    "                item.append(item_text)\n",
    "    f.close()\n",
    "    return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_items = divide_items(caricom)\n",
    "items_total = len(text_items)\n",
    "print(f'There are {len(text_items)} items in total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is one text item:\\n{text_items[random.randrange(len(text_items))]}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities Recognition with NER Stanford \n",
    "The first objective is to extract information of interest from the text. In this case we are interested in person's names, locations and activities. The first step towards this goal is to use Named Entities Recognition to recognize which words contain the information we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford NER \n",
    "NER_FOLDER = './NER-Standford/stanford-ner-2020-11-17'\n",
    "CLASSIFIER_PATH = NER_FOLDER+'/classifiers/'\n",
    "JAR_PATH = NER_FOLDER+'/stanford-ner.jar'\n",
    "\n",
    "#classifiers\n",
    "classifier_3 = 'english.all.3class.distsim.crf.ser.gz'#3 class model for recognizing locations, persons, and organizations\n",
    "classifier_4 = 'english.conll.4class.distsim.crf.ser.gz'#4 class model for recognizing locations, persons, organizations, and miscellaneous entities\n",
    "classifier_7 = 'english.muc.7class.distsim.crf.ser.gz' #7 class model for recognizing locations, persons, organizations, times, money, percents, and dates\n",
    "\n",
    "st = StanfordNERTagger(CLASSIFIER_PATH+classifier_7, JAR_PATH, encoding='utf-8')\n",
    "\n",
    "#Text retrieving\n",
    "DATA_FOLDER = './data/'\n",
    "caricom_sample = DATA_FOLDER +'Caricom_Archive_Sample_Schema1.txt'\n",
    "caricom = DATA_FOLDER +'Caricom_Archive.txt'\n",
    "\n",
    "#Extracting named-entities\n",
    "text = open(caricom_sample, 'r').read()\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the whole text is tagged. However the entities aren't grouped together. For example, a person full name is separate into two tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO tagging for readable Named Entities (i.e. regrouped NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BIO](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) tags are a way to regroup tokens, to make the output more readable. \n",
    "A person name with first and last name should be regroup by assigning  \n",
    " -B to the beginning of named entities  \n",
    " -I assigned to inside  \n",
    " -O assigned to other  \n",
    "This is done by checking the tokens just before and after the one of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function imported from \n",
    "# https://pythonprogramming.net/using-bio-tags-create-named-entity-lists/?completed=/testing-stanford-ner-taggers-for-speed/\n",
    "\n",
    "# Tag tokens with standard NLP BIO tags\n",
    "def bio_tagger(ne_tagged):\n",
    "\t\tbio_tagged = [] #empty list\n",
    "\t\tprev_tag = \"O\" #starting with a O tag\n",
    "\t\tfor token, tag in ne_tagged:\n",
    "\t\t\tif tag == \"O\": #O\n",
    "\t\t\t\tbio_tagged.append((token, tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "\t\t\t\tbio_tagged.append((token, \"I-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\treturn bio_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bio_text = bio_tagger(classified_text)\n",
    "bio_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BIO tags we can recreate a tokens list with regrouped/readable named entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function imported from \n",
    "# https://pythonprogramming.net/using-bio-tags-create-named-entity-lists/?completed=/testing-stanford-ner-taggers-for-speed/\n",
    "\n",
    "# Create tree       \n",
    "def stanford_tree(bio_tagged):\n",
    "\ttokens_raw, ne_tags = zip(*bio_tagged)\n",
    "\ttokens = [word for word in tokens_raw if word]\n",
    "\tpos_tags = [pos for token, pos in pos_tag(tokens)]\n",
    "\n",
    "\tconlltags = [(token, pos, ne) for token, pos, ne in zip(tokens, pos_tags, ne_tags)]\n",
    "\tne_tree = conlltags2tree(conlltags) #from BIO to tree format\n",
    "\treturn ne_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_text = stanford_tree(bio_text)\n",
    "tree_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function imported from \n",
    "# https://pythonprogramming.net/using-bio-tags-create-named-entity-lists/?completed=/testing-stanford-ner-taggers-for-speed/\n",
    "\n",
    "# Parse named entities from tree\n",
    "def structure_ne(ne_tree):\n",
    "\tne = []\n",
    "\tfor subtree in ne_tree:\n",
    "\t\tif type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "\t\t\tne_label = subtree.label()\n",
    "\t\t\tne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "\t\t\tne.append((ne_string, ne_label))\n",
    "\t\telse:\n",
    "\t\t\tne_label = 'O'\n",
    "\t\t\tne_string = subtree[0]\n",
    "\t\t\tne.append((ne_string, ne_label))           \n",
    "\treturn ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_ne = structure_ne(tree_text)\n",
    "clean_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_text(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    bio_text = bio_tagger(classified_text)\n",
    "    tree_text = stanford_tree(bio_text)\n",
    "    ner_item = structure_ne(tree_text)\n",
    "    return ner_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From NE tree to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure NE list for each text is transformed into an entry in a dataframe. The goal is to have for each sample of text an entry with the *relevant* informations.  \n",
    "The difficult part is to sort the relevant informations. Which of the persons is the one of interest? Which location is the location where the organization or the person was involved? Which dates are the dates of interest? \n",
    "Here we deal only with the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use schema 1 **(*name* (date) from *origin*)** to retrieve JSON names, origins and dates attributes in the text item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:\n",
    "#Output: \n",
    "#Requirements: \n",
    "#Description: \n",
    "def is_date(dateString):\n",
    "    return any(s.isdigit() for s in dateString)\n",
    "#Works for (1731-1820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: item is a single entry from text source 1 with NER tags (characterized by the '=>' starting string)\n",
    "#Output: True is the text is structured as schema 1, False otherwise\n",
    "#Requirements: is_date() function\n",
    "#Description: Test if the first elements of a text match the schema 1. Namely, does the first words match the  **Name** (*date*) from *city* pattern.\n",
    "def schema1_test(item):\n",
    "    tags = [x[1] for x in item]\n",
    "    text_middle= [x[0] for x in item]\n",
    "    #start and end of piece of interest\n",
    "    schema1 = False\n",
    "    try:\n",
    "        person_Index = tags.index('PERSON')\n",
    "    except ValueError:\n",
    "        person_Index = 1 #default\n",
    "        print(\"List does not contain value\")\n",
    "    try: \n",
    "        location_Index = tags.index('LOCATION')\n",
    "    except ValueError:\n",
    "        print(\"List does not contain value\")\n",
    "        location_Index = 0 #default\n",
    "    if person_Index < location_Index:\n",
    "        ner_middle = item[person_Index+1:location_Index-1]\n",
    "    #digit test\n",
    "    digit_test = any(x.isdigit() for x in text_middle)\n",
    "    #parenthesis test\n",
    "    if digit_test :\n",
    "        schema1 = ('(' and ')') in text_middle#parenthesis test\n",
    "\n",
    "    return schema1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function test\n",
    "schema1_test(ner_text(text_items[80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute how many items follow the **(*name* (date) from *origin*)** schema (schema 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_items = []\n",
    "for item in text_items:\n",
    "    ner_item = ner_text(item)\n",
    "    ner_items.append(ner_item)\n",
    "len(ner_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_items= []\n",
    "i = 0\n",
    "s1 = 0\n",
    "for item in ner_items:\n",
    "    #print(ner_item[0:8])\n",
    "    #print(str(i)+'\\n')\n",
    "    i = i+1\n",
    "    if schema1_test(item):\n",
    "        s1 = s1 + 1\n",
    "        print(f'Total schema1 found: {s1}')\n",
    "        s1_items.append(ner_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the new function we found 322 items following schema 1.\n",
      "\n",
      "One example is : [('=', 'O'), ('>', 'O'), ('Johann Viktor Travers von Ortenstein ( 1721–1776 )', 'PERSON'), (',', 'O'), ('of', 'O'), ('a', 'O'), ('noble', 'O'), ('family', 'O'), ('from', 'O'), ('TumeglDomleschg', 'O'), (',', 'O'), ('entered', 'O'), ('his', 'O'), ('father', 'O'), ('’', 'O'), ('s', 'O'), ('regiment', 'O'), ('in', 'O'), ('Valenciennes', 'LOCATION'), ('.', 'O'), ('After', 'O'), ('a', 'O'), ('military', 'O'), ('career', 'O'), ('in', 'O'), ('the', 'O'), ('Swiss', 'O'), ('Guards', 'O'), (',', 'O'), ('he', 'O'), ('became', 'O'), ('brigadier-general', 'O'), ('(', 'O'), ('1747', 'O'), (')', 'O'), (',', 'O'), ('marshal', 'O'), ('(', 'O'), ('1759', 'DATE'), (')', 'O'), ('and', 'O'), ('lieutenant-general', 'O'), ('(', 'O'), ('1762', 'O'), (')', 'O'), (',', 'O'), ('and', 'O'), ('was', 'O'), ('ennobled', 'O'), ('by', 'O'), ('Louis XVI', 'PERSON'), ('(', 'O'), ('«', 'O'), ('comte', 'O'), ('»', 'O'), (',', 'O'), ('1775', 'DATE'), (')', 'O'), ('.', 'O'), ('In', 'O'), ('17751776', 'O'), (',', 'O'), ('he', 'O'), ('offered', 'O'), ('to', 'O'), ('raise', 'O'), ('a', 'O'), ('Swiss', 'O'), ('regiment', 'O'), ('for', 'O'), ('the', 'O'), ('colonies', 'O'), ('.', 'O'), ('He', 'O'), ('acquired', 'O'), ('the', 'O'), ('episcopal', 'O'), ('castle', 'O'), ('and', 'O'), ('estate', 'O'), ('of', 'O'), ('«', 'O'), ('Horn', 'O'), ('»', 'O'), ('near', 'O'), ('Constance ( Germany )', 'PERSON'), (',', 'O'), ('but', 'O'), ('spent', 'O'), ('his', 'O'), ('final', 'O'), ('years', 'O'), ('in', 'O'), ('Paris', 'LOCATION'), ('.', 'O')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1_tot = len(s1_items)\n",
    "print(f'With the new function we found {s1_tot} items following schema 1.\\n\\nOne example is : {s1_items[6]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of items following schema 1 are 69%.\n"
     ]
    }
   ],
   "source": [
    "perc_s1 = s1_tot/items_total*100\n",
    "print(f'The amount of items following schema 1 are {perc_s1:2.0f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additional items starting with date and not passing shcema 1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=> In 1677, Swiss medical doctor Felix Christian Spoerri (1615-1680) from Zurich wrote a detailed description of Barbados («Americanische Reiss-Beschreibung nach den Caribes Insslen, und Neu-Engelland»), which he had visited in 1661 and 1662, including the slavery economy, which produced sugar, tobacco, cotton, and indigo.\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_items[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_items= []\n",
    "c = 0\n",
    "for item in ner_items:\n",
    "    tag = item[3][1]\n",
    "    if tag == 'DATE':\n",
    "        c = c+1\n",
    "        #print(f'items start with date : {c}')\n",
    "        date_items.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items starting with date, i.e. In 1781....: \n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of items starting with date, i.e. In 1781....: \\n{len(date_items)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional = []\n",
    "for i in date_items:\n",
    "    if not schema1_test(i):\n",
    "        c = c+1\n",
    "        additional.append(i)\n",
    "        print(f'start with date and not s1: {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('=', 'O'),\n",
       " ('>', 'O'),\n",
       " ('In', 'O'),\n",
       " ('August 1772', 'DATE'),\n",
       " (',', 'O'),\n",
       " ('Emanuel Correvon', 'ORGANIZATION'),\n",
       " ('(', 'O'),\n",
       " ('t', 'O'),\n",
       " (')', 'O'),\n",
       " ('from', 'O'),\n",
       " ('a', 'O'),\n",
       " ('Swiss', 'O'),\n",
       " ('family', 'O'),\n",
       " ('(', 'O'),\n",
       " ('either', 'O'),\n",
       " ('from', 'O'),\n",
       " ('Geneva', 'LOCATION'),\n",
       " ('or', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Canton', 'LOCATION'),\n",
       " ('of', 'O'),\n",
       " ('BerneVaud', 'O'),\n",
       " (')', 'O'),\n",
       " ('left', 'O'),\n",
       " ('for', 'O'),\n",
       " ('Berbice', 'ORGANIZATION'),\n",
       " ('.', 'O'),\n",
       " ('He', 'O'),\n",
       " ('was', 'O'),\n",
       " ('in', 'O'),\n",
       " ('debt', 'O'),\n",
       " ('of', 'O'),\n",
       " ('f', 'O'),\n",
       " ('4,000', 'O'),\n",
       " ('guilders', 'O'),\n",
       " (',', 'O'),\n",
       " ('so', 'O'),\n",
       " ('he', 'O'),\n",
       " ('might', 'O'),\n",
       " ('have', 'O'),\n",
       " ('gone', 'O'),\n",
       " ('to', 'O'),\n",
       " ('Berbice', 'LOCATION'),\n",
       " ('as', 'O'),\n",
       " ('a', 'O'),\n",
       " ('soldier', 'O'),\n",
       " ('or', 'O'),\n",
       " ('a', 'O'),\n",
       " ('plantation', 'O'),\n",
       " ('obverseer', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item starts with date that not pass schema 1 test: \n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(f'Item starts with date that not pass schema 1 test: \\n{len(additional)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of items following schema 1 are 77%.\n"
     ]
    }
   ],
   "source": [
    "perc_withAdd = (s1_tot + len(additional))/items_total*100\n",
    "print(f'The amount of items with additional test are {perc_withAdd:2.0f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspections of schema 1 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From NER to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: item is a single entry from text source 1 with NER tags (characterized by the '=>' starting string)\n",
    "#Output: A JSON string with Person,Date,Location keys if is the text is structured as schema 1, None otherwise\n",
    "#Requirements: is_date() function\n",
    "#Description: Test if the first elements of a text match the schema 1. \n",
    "#Namely, does the first words match the  **Name** (*date*) from *city* pattern.\n",
    "#If it matches schema1 it returns a dictionary \n",
    "def schema1_JSON(item):\n",
    "    s1item_JSON = None\n",
    "    tags = [x[1] for x in item]\n",
    "    text = [x[0] for x in item]\n",
    "    \n",
    "    #start and end of piece of interest, i.e. 'PERSON'.....'LOCATION'\n",
    "    schema1 = False\n",
    "    try:\n",
    "        person_Index = tags.index('PERSON')\n",
    "    except ValueError:\n",
    "        person_Index = -1 #default\n",
    "        print(\"Item does not contain a PERSON value\")\n",
    "   \n",
    "    try: \n",
    "        location_Index = tags.index('LOCATION')\n",
    "    except ValueError:\n",
    "        print(\"Item does not contain a LOCATION value\")\n",
    "        location_Index = -1 #default\n",
    "   \n",
    "    #If there are PERSON and LOCATION values, with PERSON first we continue the schema1 test\n",
    "    if person_Index < location_Index and person_Index > 0 and location_Index > 0 :\n",
    "        #define part in between PER and LOC tags\n",
    "        ner_middle = item[person_Index+1:location_Index-1]\n",
    "        text_middle = [x[0] for x in ner_middle]\n",
    "        \n",
    "        #parenthesis test\n",
    "        try:\n",
    "            par1_Index = text_middle.index('(')\n",
    "        except ValueError:\n",
    "            par1_Index = -1 #default\n",
    "        print(\"par 1 index\" + str(par1_Index))\n",
    "              \n",
    "        try:\n",
    "            par2_Index = text_middle.index(')')\n",
    "        except ValueError:\n",
    "            par2_Index = -1 #default\n",
    "        print(\"par 2 index\" + str(par2_Index))\n",
    "    \n",
    "        if par1_Index < par2_Index and par2_Index >= 0 and par1_Index >= 0 :\n",
    "            date_par = text_middle[par1_Index+1:par2_Index]\n",
    "            print('This is the text in between parenthesis ' +str(date_par))\n",
    "            #digit test\n",
    "            digit_test = any(x.isdigit() for x in str(date_par))\n",
    "            print('The digit test results : '+str(digit_test))\n",
    "            #Save informations from schema 1\n",
    "            if digit_test :\n",
    "                \n",
    "                #retrieve date\n",
    "                date = ''\n",
    "                date_split = str(date_par).split('–')\n",
    "                for x in str(date_split):\n",
    "                    if x.isdigit():\n",
    "                        date = date +' '+ x\n",
    "                print('The retrieved date is ' + date)\n",
    "                #retrieve location and person values\n",
    "                location = item[location_Index][0]\n",
    "                person = item[person_Index][0]\n",
    "        \n",
    "            #Create a JSON dictionary\n",
    "                s1item_JSON = {\n",
    "                    'person' : person,\n",
    "                    'date': date,\n",
    "                    'location': location,\n",
    "                #'field':NA\n",
    "                }\n",
    "    return s1item_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function test\n",
    "n = 30 \n",
    "print(text_items[n])\n",
    "schema1_JSON(ner_text(text_items[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(x.isdigit() for x in '1685–1740')\n",
    "'1685–1740'.split('–')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON from schema 1 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_JSON= []\n",
    "i = 0\n",
    "s1 = 0\n",
    "for item in text_items:\n",
    "    ner_item = ner_text(item)\n",
    "    print(ner_item[0:8])\n",
    "    #print(str(i)+'\\n')\n",
    "    i = i+1\n",
    "    json = schema1_JSON(ner_item)\n",
    "    print(json)\n",
    "    s1_JSON.append(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_JSON_clean = [] \n",
    "for val in s1_JSON:\n",
    "    if val != None :\n",
    "        s1_JSON_clean.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_JSON_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceptions, errors etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some have '>' as person \n",
    "len(s1_JSON_clean)\n",
    "#s1_JSON_clean[1]\n",
    "\n",
    "#some have 'Canton' or 'City' instead of location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate\n",
    "If some entries have the samed person we need to merge or remove one of the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_items[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get location when mentioned further to deal with : from the city of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use section name to retrieve JSON colonial location attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use predefined categories to retrieve the JSON type attribute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old version of schema 1 test\n",
    "This version is outdated. To restrictive it gets only 18 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: item is a single entry from text source 1 with NER tags (characterized by the '=>' starting string)\n",
    "#Output: True is the text is structured as schema 1, False otherwise\n",
    "#Requirements: is_date() function\n",
    "#Description: Test if the first elements of a text match the schema 1. Namely, does the first words match the  **Name** (*date*) from *city* pattern.\n",
    "def schema1_test(item): \n",
    "    testValue = (item[2][1] == ('PERSON' or 'ORGANIZATION)')) and (item[3][0] == '(') and (is_date(item[4][0]) == True) and (item[5][0] == ')') and (item[6][0] == 'from') and (item[7][1] == 'LOCATION')\n",
    "    return testValue\n",
    "\n",
    "schema1_test(clean_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about multiple persons in a paragraph?\n",
    "    -> one ID per person with same organization groups etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.DataFrame({\n",
    "                     'id':[],\n",
    "                     'person':[],\n",
    "                     'location':[],\n",
    "                     'period':[],})\n",
    "dataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text)\n",
    "        \n",
    "person_counts = Counter(person_list).most_common(20)\n",
    "df_person = pd.DataFrame(person_counts, columns =['text', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
